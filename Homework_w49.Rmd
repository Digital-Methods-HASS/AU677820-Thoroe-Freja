---
title: "Scrapping Lynchings in the US data with R"
author: "Freja Skall Thor√∏e"
date: "5/12/2021, updated `r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Goal and Assignment
2.2) use the 'rvest' library to scrape data of your interest (football statistics in Wikipedia? trends of self-inflicted death in musicians? global population by country in https://www.worldometers.info/world-population/population-by-country/ )

In this project in want to look at some data on lynching in the US from 1882 to 1968 and try to make it easy to analyze. I get my data from a webpage that has it's information from the Archive of Tuskegee Institute, that no longer store the lynching statistics. 

##Challenge
The data is simple and not to challenging at first glance. However the simplicity of the data might be a challange to work with using the excact same method as Adela in her scrapping example. 

Link to data: (http://law2.umkc.edu/faculty/projects/ftrials/shipp/lynchingyear.html)

# Solution
First, install a handful of classic R packages and load their libraries:

- `rvest` for web-scraping
- `dplyr` for data-wrangling
- `tidyr` for data transformation
- `stringr` for string manipulation
- `janitor` for clean headers that your OCD will love you for


```{r libraries, warning=FALSE, message=FALSE}
library(rvest)
library(dplyr)
library(tidyr)
library(stringr)
library(janitor)
library(purrr)
```
## Scrape the data

Next, learn how scrape the content of the website and extract the HTML table:
```{r url}
url_Lynching <- "http://law2.umkc.edu/faculty/projects/ftrials/shipp/lynchingyear.html"
# scrape the website
url_html_lynching <- read_html(url_Lynching)
```

I first tried Option 1 where you extract the individual rows from the HTML table by using the tag <tr> as argument in the html_nodes() function. This makes each row into an element in a character vector, which would require extra cleaning, specifically splitting by newline \n. *This did not work on my html*. 

```{r scrape-rows}
whole_table <- url_html_lynching %>% 
 html_nodes("tr") %>%
 html_text(trim = FALSE) 
head(whole_table)
```

I then tried option 2 where you extract individual cells from the HTML table by using the tag <td> as an argument in the html_nodes() function. This creates a character element out of each table cell, which would require extra wrangling; specifically concatenating by timestamp...*This did not work either*.

```{r scrape-cells}
whole_table <- url_html_lynching %>% 
 html_nodes("td") %>%
 html_text(trim = TRUE) 
head(whole_table)
```


I then tried to use Adelas option 3 to extract the whole HTML table through the <table> tag. It loads the html table into a list not a dataframe, but we can unlist the resulting list and coerce it into a dataframe, and that's less work than options 1 and 2 above. *however this did not really work either, since the data in my html seems to be simple without any CSS (i think its called, I might be totally wrong)*

```{r scrape-table}
whole_table <- url_html_lynching %>%
 html_nodes("table") %>%
 html_table()  #str(whole_table) turns out to be a list
head(whole_table)
```
ed. I loaded my dataset again and for some reason it worked. 

I then tried to look for solutions online. I found this tutorial that I tried to follow: https://www.youtube.com/watch?v=smriIBG08ok 

I load in my URL again and add a "2" in the title to mark it my second attempt. 

I found a new library for missing values: install.packages('xml2')

```{r}
#install.packages('xml2') 
library(xml2)
```


```{r}
url_Lynching2 <- "http://law2.umkc.edu/faculty/projects/ftrials/shipp/lynchingyear.html"

text <- map(url_Lynching2, ~html_node(., 'div')) 
text <- lapply(url_Lynching2, function(x) html_node(x, "div")) #none of these worked as solution to the code error from the code below


#First scraping the title
url_Lynching2 %>% #I kept getting an error in UseMethod("xml_find_first"):... so I searched the web again - upd. installing the xml2 package helped.
	html_node("p") %>% 
	html_text()

# I for some reason am getting the same errormessage over and over again. And the xm12 package is not working anymore. 

```

Since the codes above did not work either, I search the web again. 
I tried to follow a tutorial where he used CSS selecters in chrome to select what he wanted to scrape, but that did not work on my hmtl. Link to tutorial: https://www.youtube.com/watch?v=v8Yh_4oE-Fs&t=183s 


```{r html-to-df}
new_table <- do.call(cbind,unlist(whole_table, recursive = FALSE)) 
	head(new_table) # Looks horrible, i need to get rid of that \n and put it all in order, but I have no idea how to do that.
```
I am not sure I have to make a function the works on all tables, since there is only one. But now I do it just to make sure. Since i it would take too much work to clean up the webscraping I decided to just load the data into an excelsheet and work on that instead. 


I decided to give up on doing this task since i spend 3+ hours trying to figure out why my data got so messed up. I know that you would have to get rid of the newline markers \n. But it would take me too long to figure out, so i will turn this in. I will not be able to knit the file either because of the errors in the document. 

